{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encodgin='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "%matplotlib inline\n",
    "\n",
    "# Use a white background for matplotlib figures\n",
    "matplotlib.rcParams['figure.facecolor'] = '#ffffff'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to /Git2020_12_20_Eunjoo/Code_Practice_2020/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f55127e2edf34ebab877ae2130eba6e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /Git2020_12_20_Eunjoo/Code_Practice_2020/MNIST/raw/train-images-idx3-ubyte.gz to /Git2020_12_20_Eunjoo/Code_Practice_2020/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to /Git2020_12_20_Eunjoo/Code_Practice_2020/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "819df4a22845440b8aa1f5adf6d6d5fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /Git2020_12_20_Eunjoo/Code_Practice_2020/MNIST/raw/train-labels-idx1-ubyte.gz to /Git2020_12_20_Eunjoo/Code_Practice_2020/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to /Git2020_12_20_Eunjoo/Code_Practice_2020/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dca9ff41ff4d40daa4d81aa699709028",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /Git2020_12_20_Eunjoo/Code_Practice_2020/MNIST/raw/t10k-images-idx3-ubyte.gz to /Git2020_12_20_Eunjoo/Code_Practice_2020/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to /Git2020_12_20_Eunjoo/Code_Practice_2020/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e00ae2022bf4187a8ab0557edaee85c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /Git2020_12_20_Eunjoo/Code_Practice_2020/MNIST/raw/t10k-labels-idx1-ubyte.gz to /Git2020_12_20_Eunjoo/Code_Practice_2020/MNIST/raw\n",
      "Processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/torchvision/datasets/mnist.py:480: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:141.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "dataset = MNIST(root='/Git2020_12_20_Eunjoo/Code_Practice_2020', download=True, transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image.shape: torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "image,label = dataset[0]\n",
    "print('image.shape:', image.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's use the random_split helper function to set aside 10000 images for our validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 10000)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_size = 10000\n",
    "train_size =len(dataset)-val_size\n",
    "\n",
    "train_ds, val_ds =random_split(dataset, [train_size, val_size])\n",
    "len(train_ds),len(val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create PyTorch data loaders for training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x1312c56a0>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x1312c5100>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader = DataLoader(train_ds,\n",
    "                          batch_size, shuffle=True, num_workers=4,\n",
    "                          pin_memory=True)\n",
    "val_loader = DataLoader(val_ds, batch_size*2, num_workers=4, pin_memory=True)\n",
    "train_loader,val_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize a batch of data in a grid using the make_grid function from torchvision. We'll also use the .permute method on the tensor to move the channels to the last dimension, as expected by matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images,shape: torch.Size([128, 1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for images, _ in train_loader:\n",
    "    print('images,shape:', images.shape)\n",
    "    plt.figure(figsize=(16,8))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(make_grid(image,nrow=16).permute((1,2,0)))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hidden Layers, Activation Functions and Non-Linearity\n",
    "how hidden layers and activation functions can help capture non-linear relationships between inputs and outputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's create a batch of inputs tensors. We'll flatten the 1x28x28 images into vectors of size 784, so they can be passed into an nn.Linear object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images.shape: torch.Size([128, 1, 28, 28])\n",
      "imputs.shape: torch.Size([128, 784])\n"
     ]
    }
   ],
   "source": [
    "for images, labels in train_loader:\n",
    "    print('images.shape:', images.shape)\n",
    "    inputs = images.reshape(-1,784)\n",
    "    print('imputs.shape:',inputs.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's create a nn.Linear object, which will serve as our hidden layer. We'll set the size of the output from the hidden layer to 64. This number can be increased or decreased to change the learning capacity of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = inputs.shape[-1]\n",
    "hidden_size =64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1 = nn.Linear(input_size, hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compute intermediate outputs for the batch of images by passing inputs through layer1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 784])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer1_outputs.shape: torch.Size([128, 64])\n"
     ]
    }
   ],
   "source": [
    "layer1_outputs = layer1(inputs)\n",
    "print('layer1_outputs.shape:',layer1_outputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image vectors of size 784 are transformed into intermediate output vectors of length 32 by performing a matrix multiplication of inputs matrix with the transposed weights matrix of layer1 and adding the bias. We can verify this using torch.allclose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 64])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer1_outputs_direct = inputs @layer1.weight.t()+layer1.bias\n",
    "layer1_outputs_direct.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(layer1_outputs, layer1_outputs_direct, 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, layer1_outputs and inputs have a linear relationship, i.e., each element of layer_outputs is a weighted sum of elements from inputs. Thus, even as we train the model and modify the weights, layer1 can only capture linear relationships between inputs and outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use the F.relu method to apply ReLU to the elements of a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.2000, 3.0000]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.relu(torch.tensor([[1,-2,0],[-0.1,.2,3]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "apply the activation function to layer1_outputs and verify that negative values were replaced with 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min(layer1_outputs): -0.6810014843940735\n",
      "min(relu_outputs): 0.0\n"
     ]
    }
   ],
   "source": [
    "relu_outputs=F.relu(layer1_outputs)\n",
    "print('min(layer1_outputs):',torch.min(layer1_outputs).item())\n",
    "print('min(relu_outputs):',torch.min(relu_outputs).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's create an output layer to convert vectors of length hidden_size in relu_outputs into vectors of length 10, which is the desired output of our model (since there are 10 target labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_size =10\n",
    "layer2 =nn.Linear(hidden_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2238,  0.0979,  0.0921,  ...,  0.0119, -0.1816,  0.0997],\n",
      "        [-0.2256,  0.2145,  0.2192,  ..., -0.1529, -0.1292,  0.0598],\n",
      "        [-0.1963,  0.0092,  0.0234,  ..., -0.0303, -0.2244,  0.0883],\n",
      "        ...,\n",
      "        [-0.2208,  0.1983,  0.2024,  ..., -0.0803, -0.1269,  0.0901],\n",
      "        [-0.2486,  0.2485,  0.2301,  ..., -0.1206, -0.0775, -0.0417],\n",
      "        [-0.1927,  0.1583,  0.1325,  ..., -0.0321, -0.0854, -0.0111]],\n",
      "       grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "layer2_ouputs=layer2(relu_outputs)\n",
    "print(layer2_ouputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "layer2_outputs contains a batch of vectors of size 10. We can now use this output to compute the loss using F.cross_entropy and adjust the weights of layer1 and layer2 using gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3206, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cross_entropy(layer2_ouputs,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "our model transforms inputs into layer2_outputs by applying a linear transformation (using layer1), followed by a non-linear activation (using F.relu), followed by another linear transformation (using layer2). Let's verify this by re-computing the output using basic matrix operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expanded version of layer2(F.relu(layer1(inputs)))\n",
    "outputs =( F.relu(inputs@layer1.weight.t()+layer1.bias))@layer2.weight.t()+layer2.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(outputs,layer2_ouputs,1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that outputs and inputs do not have a linear relationship due to the non-linear activation function F.relu. As we train the model and adjust the weights of layer1 and layer2, we can now capture non-linear relationships between the images and their labels. In other words, introducing non-linearity makes the model more powerful and versatile. Also, since hidden_size does not depend on the dimensions of the inputs or outputs, we vary it to increase the number of parameters within the model. We can also introduce new hidden layers and apply the same non-linear activation after each hidden layer.\n",
    "The model we just created is called a neural network. A deep neural network is simply a neural network with one or more hidden layers. In fact, the Universal Approximation Theorem states that a sufficiently large & deep neural network can compute any arbitrary function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's hard to imagine how the simple process of multiplying inputs with randomly initialized matrices, applying non-linear activations, and adjusting weights repeatedly using gradient descent can yield such astounding results. Deep learning models often contain millions of parameters, which can together capture far more complex relationships than the human brain can comprehend.\n",
    "If we hadn't included a non-linear activation between the two linear layers, the final relationship between inputs and outputs would still be linear. A simple refactoring of the computations illustrates this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1330,  0.0232,  0.1974,  ...,  0.0487, -0.1887,  0.0902],\n",
       "        [-0.1612,  0.1909,  0.3098,  ..., -0.1813, -0.0664, -0.0511],\n",
       "        [-0.1860, -0.0602,  0.0680,  ...,  0.0558, -0.2296,  0.0656],\n",
       "        ...,\n",
       "        [-0.1352,  0.1092,  0.3813,  ..., -0.0292, -0.1088, -0.0266],\n",
       "        [-0.2414,  0.2746,  0.2559,  ..., -0.0635, -0.0474, -0.0908],\n",
       "        [-0.1966,  0.0917,  0.1957,  ...,  0.0732, -0.0874, -0.0846]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Same as layer2(layer1(inputs))\n",
    "outputs2 = (inputs@layer1.weight.t()+layer1.bias)@layer2.weight.t()+layer2.bias\n",
    "outputs2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-6.8571e-03, -1.1927e-02,  1.7962e-02,  ...,  4.5137e-03,\n",
       "          -1.8537e-02, -3.4834e-03],\n",
       "         [-1.2102e-02,  7.4426e-03, -1.9773e-02,  ..., -1.6559e-02,\n",
       "          -4.5197e-05, -1.5673e-03],\n",
       "         [-1.4430e-02, -2.5943e-03,  1.3884e-02,  ..., -7.2665e-03,\n",
       "          -1.7290e-02, -2.8490e-02],\n",
       "         ...,\n",
       "         [-4.8138e-03,  5.7340e-03,  7.6974e-03,  ..., -5.1753e-03,\n",
       "           2.4489e-02,  4.8596e-03],\n",
       "         [-1.0800e-02, -9.1551e-03, -1.8609e-02,  ..., -1.2222e-02,\n",
       "           2.1270e-03, -2.8671e-03],\n",
       "         [ 1.3171e-02, -5.6174e-03,  5.7840e-03,  ...,  2.5876e-02,\n",
       "           2.1001e-03, -4.7757e-03]]),\n",
       " tensor([-0.1221,  0.1194,  0.1183, -0.0443, -0.0637,  0.0495, -0.0152, -0.0665,\n",
       "         -0.1015,  0.0421]))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a single layer to replace the two linear layers\n",
    "combined_layer = nn.Linear(input_size, output_size)\n",
    "combined_layer.weight.data= layer2.weight@layer1.weight\n",
    "combined_layer.bias.data=layer1.bias@layer2.weight.t()+layer2.bias\n",
    "combined_layer.weight.data,combined_layer.bias.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1330,  0.0232,  0.1974,  ...,  0.0487, -0.1887,  0.0902],\n",
       "        [-0.1612,  0.1909,  0.3098,  ..., -0.1813, -0.0664, -0.0511],\n",
       "        [-0.1860, -0.0602,  0.0680,  ...,  0.0558, -0.2296,  0.0656],\n",
       "        ...,\n",
       "        [-0.1352,  0.1092,  0.3813,  ..., -0.0292, -0.1088, -0.0266],\n",
       "        [-0.2414,  0.2746,  0.2559,  ..., -0.0635, -0.0474, -0.0908],\n",
       "        [-0.1966,  0.0917,  0.1957,  ...,  0.0732, -0.0874, -0.0846]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs3 = inputs@combined_layer.weight.t()+combined_layer.bias\n",
    "outputs3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(outputs2, outputs3, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the library\n",
    "!pip install jovian --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Attempting to save notebook..\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m[jovian] Error: The detected/provided file \"3_lesson_practice_by using other dataset.ipynb\" does not exist. Please provide the correct notebook filename as the \"filename\" argument to \"jovian.commit\".\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import jovian\n",
    "jovian.commit(project='04-feedforward-nn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
